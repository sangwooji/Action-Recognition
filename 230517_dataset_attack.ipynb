{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a2459b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Normalization parameters for pre-trained PyTorch models\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, dataset_path, split_path, split_number, input_shape, sequence_length, training):\n",
    "        self.training = training\n",
    "        self.label_index = self._extract_label_mapping(split_path)\n",
    "        self.sequences = self._extract_sequence_paths(dataset_path, split_path, split_number, training)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.label_names = sorted(list(set([self._activity_from_path(seq_path) for seq_path in self.sequences])))\n",
    "        self.num_classes = len(self.label_names)\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(input_shape[-2:], Image.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _extract_label_mapping(self, split_path=\"data/ucfTrainTestlist\"):\n",
    "        \"\"\" Extracts a mapping between activity name and softmax index \"\"\"\n",
    "        with open(os.path.join(split_path, \"classInd.txt\")) as file:\n",
    "            lines = file.read().splitlines()\n",
    "        label_mapping = {}\n",
    "        for line in lines:\n",
    "            label, action = line.split()\n",
    "            label_mapping[action] = int(label) - 1\n",
    "        return label_mapping\n",
    "\n",
    "    def _extract_sequence_paths(\n",
    "        self, dataset_path, split_path=\"data/ucfTrainTestlist\", split_number=1, training=True\n",
    "    ):\n",
    "        \"\"\" Extracts paths to sequences given the specified train / test split \"\"\"\n",
    "        assert split_number in [1, 2, 3], \"Split number has to be one of {1, 2, 3}\"\n",
    "        fn = f\"trainlist0{split_number}.txt\" if training else f\"testlist0{split_number}.txt\"\n",
    "        split_path = os.path.join(split_path, fn)\n",
    "        with open(split_path) as file:\n",
    "            lines = file.read().splitlines()\n",
    "        sequence_paths = []\n",
    "        for line in lines:\n",
    "            seq_name = line.split(\".avi\")[0]\n",
    "            sequence_paths += [os.path.join(dataset_path, seq_name)]\n",
    "        return sequence_paths\n",
    "\n",
    "    def _activity_from_path(self, path):\n",
    "        \"\"\" Extracts activity name from filepath \"\"\"\n",
    "        return path.split(\"/\")[-2]\n",
    "\n",
    "    def _frame_number(self, image_path):\n",
    "        \"\"\" Extracts frame number from filepath \"\"\"\n",
    "        return int(image_path.split(\"/\")[-1].split(\".jpg\")[0])\n",
    "\n",
    "    def _pad_to_length(self, sequence):\n",
    "        \"\"\" Pads the sequence to required sequence length \"\"\"\n",
    "        left_pad = sequence[0]\n",
    "        if self.sequence_length is not None:\n",
    "            while len(sequence) < self.sequence_length:\n",
    "                sequence.insert(0, left_pad)\n",
    "        return sequence\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sequence_path = self.sequences[index % len(self)]\n",
    "        # Sort frame sequence based on frame number\n",
    "        image_paths = sorted(glob.glob(f\"{sequence_path}/*.jpg\"), key=lambda path: self._frame_number(path))\n",
    "        # Pad frames sequences shorter than `self.sequence_length` to length\n",
    "        image_paths = self._pad_to_length(image_paths)\n",
    "        if self.training:\n",
    "            # Randomly choose sample interval and start frame\n",
    "            sample_interval = np.random.randint(1, len(image_paths) // self.sequence_length + 1)\n",
    "            start_i = np.random.randint(0, len(image_paths) - sample_interval * self.sequence_length + 1)\n",
    "            flip = np.random.random() < 0.5\n",
    "        else:\n",
    "            # Start at first frame and sample uniformly over sequence\n",
    "            start_i = 0\n",
    "            sample_interval = 1 if self.sequence_length is None else len(image_paths) // self.sequence_length\n",
    "            flip = False\n",
    "        # Extract frames as tensors\n",
    "        idx_sequence = []\n",
    "        image_sequence = []\n",
    "        for i in range(start_i, len(image_paths), sample_interval):\n",
    "            if self.sequence_length is None or len(image_sequence) < self.sequence_length:\n",
    "                image_tensor = self.transform(Image.open(image_paths[i]))\n",
    "                if flip:\n",
    "                    image_tensor = torch.flip(image_tensor, (-1,))\n",
    "                image_sequence.append(image_tensor)\n",
    "                idx_sequence.append(i)\n",
    "        image_sequence = torch.stack(image_sequence)\n",
    "        target = self.label_index[self._activity_from_path(sequence_path)]\n",
    "        idx_sequence = torch.tensor(idx_sequence)\n",
    "        return image_sequence, target, idx_sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "92456530",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'data/UCF-101-frames'\n",
    "split_path = 'data/ucfTrainTestlist'\n",
    "split_number = 1\n",
    "image_shape = (3, 224, 224)\n",
    "sequence_length = 40\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    dataset_path=dataset_path,\n",
    "    split_path=split_path,\n",
    "    split_number=split_number,\n",
    "    input_shape=image_shape,\n",
    "    sequence_length=sequence_length,\n",
    "    training=True,\n",
    ")\n",
    "\n",
    "test_dataset = Dataset(\n",
    "    dataset_path=dataset_path,\n",
    "    split_path=split_path,\n",
    "    split_number=split_number,\n",
    "    input_shape=image_shape,\n",
    "    sequence_length=sequence_length,\n",
    "    training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "10e9b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "import itertools\n",
    "from models import *\n",
    "from dataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1afee30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1dbba3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n"
     ]
    }
   ],
   "source": [
    "for i, (frames, labels, idx_sequence) in enumerate(test_dataloader):\n",
    "    print ('iter', i)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a84f86c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 40, 3, 224, 224])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a7c0dd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "69c16ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 40])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7f91ba42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n"
     ]
    }
   ],
   "source": [
    "for i, (frames, labels, idx_sequence) in enumerate(train_dataloader):\n",
    "    print ('iter', i)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "85c816ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   3,   6,   9,  12,  15,  18,  21,  24,  27,  30,  33,  36,  39,\n",
       "          42,  45,  48,  51,  54,  57,  60,  63,  66,  69,  72,  75,  78,  81,\n",
       "          84,  87,  90,  93,  96,  99, 102, 105, 108, 111, 114, 117],\n",
       "        [ 41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,\n",
       "          55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,\n",
       "          69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80],\n",
       "        [ 26,  28,  30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,  52,\n",
       "          54,  56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,  78,  80,\n",
       "          82,  84,  86,  88,  90,  92,  94,  96,  98, 100, 102, 104],\n",
       "        [  3,   6,   9,  12,  15,  18,  21,  24,  27,  30,  33,  36,  39,  42,\n",
       "          45,  48,  51,  54,  57,  60,  63,  66,  69,  72,  75,  78,  81,  84,\n",
       "          87,  90,  93,  96,  99, 102, 105, 108, 111, 114, 117, 120],\n",
       "        [ 73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,\n",
       "          87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100,\n",
       "         101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112],\n",
       "        [ 27,  30,  33,  36,  39,  42,  45,  48,  51,  54,  57,  60,  63,  66,\n",
       "          69,  72,  75,  78,  81,  84,  87,  90,  93,  96,  99, 102, 105, 108,\n",
       "         111, 114, 117, 120, 123, 126, 129, 132, 135, 138, 141, 144],\n",
       "        [ 61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,\n",
       "          75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,\n",
       "          89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100],\n",
       "        [ 16,  19,  22,  25,  28,  31,  34,  37,  40,  43,  46,  49,  52,  55,\n",
       "          58,  61,  64,  67,  70,  73,  76,  79,  82,  85,  88,  91,  94,  97,\n",
       "         100, 103, 106, 109, 112, 115, 118, 121, 124, 127, 130, 133],\n",
       "        [ 71,  74,  77,  80,  83,  86,  89,  92,  95,  98, 101, 104, 107, 110,\n",
       "         113, 116, 119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152,\n",
       "         155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188],\n",
       "        [ 57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
       "          71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
       "          85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96],\n",
       "        [ 10,  14,  18,  22,  26,  30,  34,  38,  42,  46,  50,  54,  58,  62,\n",
       "          66,  70,  74,  78,  82,  86,  90,  94,  98, 102, 106, 110, 114, 118,\n",
       "         122, 126, 130, 134, 138, 142, 146, 150, 154, 158, 162, 166],\n",
       "        [119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152, 155, 158,\n",
       "         161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200,\n",
       "         203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236],\n",
       "        [ 24,  26,  28,  30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,\n",
       "          52,  54,  56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,  78,\n",
       "          80,  82,  84,  86,  88,  90,  92,  94,  96,  98, 100, 102],\n",
       "        [ 13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "          27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
       "          41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52],\n",
       "        [  4,   6,   8,  10,  12,  14,  16,  18,  20,  22,  24,  26,  28,  30,\n",
       "          32,  34,  36,  38,  40,  42,  44,  46,  48,  50,  52,  54,  56,  58,\n",
       "          60,  62,  64,  66,  68,  70,  72,  74,  76,  78,  80,  82],\n",
       "        [ 41,  43,  45,  47,  49,  51,  53,  55,  57,  59,  61,  63,  65,  67,\n",
       "          69,  71,  73,  75,  77,  79,  81,  83,  85,  87,  89,  91,  93,  95,\n",
       "          97,  99, 101, 103, 105, 107, 109, 111, 113, 115, 117, 119]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba176ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fe6e9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Normalization parameters for pre-trained PyTorch models\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "class DatasetForAttack(Dataset):\n",
    "    def __init__(self, dataset_path, split_path, split_number, input_shape, sequence_length):\n",
    "        self.training = False\n",
    "        self.label_index = self._extract_label_mapping(split_path)\n",
    "        self.sequences = self._extract_sequence_paths(dataset_path, split_path, split_number, self.training)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.label_names = sorted(list(set([self._activity_from_path(seq_path) for seq_path in self.sequences])))\n",
    "        self.num_classes = len(self.label_names)\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(input_shape[-2:], Image.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _extract_label_mapping(self, split_path=\"data/ucfTrainTestlist\"):\n",
    "        \"\"\" Extracts a mapping between activity name and softmax index \"\"\"\n",
    "        with open(os.path.join(split_path, \"classInd.txt\")) as file:\n",
    "            lines = file.read().splitlines()\n",
    "        label_mapping = {}\n",
    "        for line in lines:\n",
    "            label, action = line.split()\n",
    "            label_mapping[action] = int(label) - 1\n",
    "        return label_mapping\n",
    "\n",
    "    def _extract_sequence_paths(\n",
    "        self, dataset_path, split_path=\"data/ucfTrainTestlist\", split_number=1, training=True\n",
    "    ):\n",
    "        \"\"\" Extracts paths to sequences given the specified train / test split \"\"\"\n",
    "        assert split_number in [1, 2, 3], \"Split number has to be one of {1, 2, 3}\"\n",
    "        fn = f\"trainlist0{split_number}.txt\" if training else f\"testlist0{split_number}.txt\"\n",
    "        split_path = os.path.join(split_path, fn)\n",
    "        with open(split_path) as file:\n",
    "            lines = file.read().splitlines()\n",
    "        sequence_paths = []\n",
    "        for line in lines:\n",
    "            seq_name = line.split(\".avi\")[0]\n",
    "            sequence_paths += [os.path.join(dataset_path, seq_name)]\n",
    "        return sequence_paths\n",
    "\n",
    "    def _activity_from_path(self, path):\n",
    "        \"\"\" Extracts activity name from filepath \"\"\"\n",
    "        return path.split(\"/\")[-2]\n",
    "\n",
    "    def _frame_number(self, image_path):\n",
    "        \"\"\" Extracts frame number from filepath \"\"\"\n",
    "        return int(image_path.split(\"/\")[-1].split(\".jpg\")[0])\n",
    "\n",
    "    def _pad_to_length(self, sequence):\n",
    "        \"\"\" Pads the sequence to required sequence length \"\"\"\n",
    "        left_pad = sequence[0]\n",
    "        if self.sequence_length is not None:\n",
    "            while len(sequence) < self.sequence_length:\n",
    "                sequence.insert(0, left_pad)\n",
    "        return sequence\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sequence_path = self.sequences[index % len(self)]\n",
    "        # Sort frame sequence based on frame number\n",
    "        image_paths = sorted(glob.glob(f\"{sequence_path}/*.jpg\"), key=lambda path: self._frame_number(path))\n",
    "        # Pad frames sequences shorter than `self.sequence_length` to length\n",
    "        image_paths = self._pad_to_length(image_paths)\n",
    "        if self.training:\n",
    "            assert(0)\n",
    "        else:\n",
    "            # Randomly choose sample interval and start frame\n",
    "            sample_interval = np.random.randint(1, len(image_paths) // self.sequence_length + 1)\n",
    "            start_i = np.random.randint(0, len(image_paths) - sample_interval * self.sequence_length + 1)\n",
    "            flip = np.random.random() < 0.5\n",
    "        # Extract frames as tensors\n",
    "        idx_sequence = []\n",
    "        image_sequence = []\n",
    "        for i in range(start_i, len(image_paths), sample_interval):\n",
    "            if self.sequence_length is None or len(image_sequence) < self.sequence_length:\n",
    "                image_tensor = self.transform(Image.open(image_paths[i]))\n",
    "                if flip:\n",
    "                    image_tensor = torch.flip(image_tensor, (-1,))\n",
    "                image_sequence.append(image_tensor)\n",
    "                idx_sequence.append(i)\n",
    "        image_sequence = torch.stack(image_sequence)\n",
    "        target = self.label_index[self._activity_from_path(sequence_path)]\n",
    "        idx_sequence = torch.tensor(idx_sequence)\n",
    "        return image_sequence, target, idx_sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "377cf264",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_dataset = DatasetForAttack(\n",
    "    dataset_path=dataset_path,\n",
    "    split_path=split_path,\n",
    "    split_number=split_number,\n",
    "    input_shape=image_shape,\n",
    "    sequence_length=sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "44518e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(attack_dataset, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be2e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
